<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete">

<head>
  <meta charset="UTF-8">
  <title>LIBEVOLUTIONEVAL</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Bulma and other CSS -->
  <link rel="stylesheet" href="src/bulma.min.css">
  <link rel="stylesheet" href="src/fontawesome.all.min.css">
  <link rel="stylesheet" href="src/index.css">
  
  <!-- Scripts -->
  <script src="src/jquery.min.js"></script>
  <script defer src="src/fontawesome.all.min.js"></script>
</head>

<body>

<!-- Hero Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LIBEVOLUTIONEVAL: A Benchmark and Study for Version-Specific Code Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Sachit Kuhar<sup>1</sup>,</span>
            <span class="author-block">Wasi Uddin Ahmad<sup>2</sup>*,</span>
            <span class="author-block">Zijian Wang<sup>1</sup>,</span>
            <span class="author-block">Nihal Jain<sup>1</sup>,</span>
            <span class="author-block">Haifeng Qian<sup>2</sup>*,</span>
            <span class="author-block">Baishakhi Ray<sup>1</sup>,</span>
            <span class="author-block">Murali Krishna Ramanathan<sup>1</sup>,</span>
            <span class="author-block">Xiaofei Ma<sup>1</sup>,</span>
            <span class="author-block">Anoop Deoras<sup>1</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>AWS AI Labs</span>,
            <span class="author-block"><sup>2</sup>NVIDIA</span>
            <p class="has-text-centered" style="margin-top:10px;">
              <i>* Work done at AWS AI Labs</i>
            </p>
          </div>

          <!-- Link to Paper -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.04478" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
            </div>
          </div>
          <!-- End link block -->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TL;DR Section -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="subtitle has-text-centered">
        <h2 class="title is-3 has-text-centered">TL; DR</h2>
      </div>
      <br>
      <div class="content has-text-justified">
        Recent advancements in code completion models have often overlooked the evolving nature of public libraries. 
        <b>LIBEVOLUTIONEVAL</b> bridges this gap by providing a benchmark that rigorously tests 
        Large Language Models on version-specific code completion across multiple public libraries such as 
        PyTorch, Matplotlib, SciPy, and more. Our experiments show that model performance can vary significantly 
        when libraries undergo rapid version changes—APIs might be introduced, deprecated, or modified. We demonstrate 
        that providing version-aware context, such as retrieved documentation, helps but does not fully solve the 
        inherent challenges. We also show that embedding models themselves exhibit bias toward certain library versions, 
        highlighting the complexity of handling evolving public libraries in real-world development settings.
      </div>
      <p align="center">
        <!-- teaser.png -->
        <img src="teaser.png" width="100%" align="middle" class="center" alt="[LIBEVOLUTIONEVAL Teaser Image]">
      </p>
    </div>
  </div>
</section>

<!-- High-Level Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">High-Level Overview</h2>
        <div class="content has-text-justified">
          <p>
            <b>LIBEVOLUTIONEVAL</b> specifically examines how code language models handle fast-evolving public libraries. 
            While prior benchmarks often focus on single-file or repository-level contexts, <b>LIBEVOLUTIONEVAL</b> 
            zeroes in on version-specific API calls — including scenarios where APIs are <em>introduced</em>, 
            <em>modified</em>, or <em>deprecated</em> across different releases.
          </p>
          <p>
            We study multiple Python libraries including <i>torch, torchvision, scipy, pillow, tqdm, pyyaml, matplotlib,</i> 
            and <i>pandas</i> to evaluate how modern code completion models adapt to or fail in changing API environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in code completion models have primarily focused on either single-file contexts or entire repositories, 
            overlooking the critical challenge of fast-evolving public libraries. As libraries introduce, deprecate, or modify APIs across versions, 
            a model’s performance can vary significantly based on the year and version of the library in use. 
          </p>
          <p>
            We propose <b>LIBEVOLUTIONEVAL</b>, a benchmark and detailed study that explicitly evaluates code language models under version-specific code completion tasks. 
            Spanning multiple years and covering popular Python libraries, it provides both real-world GitHub-based examples and controlled, 
            documentation-based tasks. Our experiments with popular code LLMs and embedding models reveal that addressing library evolution 
            demands version-aware context. While retrieval of version-specific documentation can improve code completions, 
            it does not fully resolve version-dependent biases in the models, indicating the need for specialized training 
            techniques to handle rapid library changes.
          </p>
          <p>
            We hope that <b>LIBEVOLUTIONEVAL</b> inspires further research that incorporates temporal and version-specific knowledge 
            into code completion models, enabling them to reflect real-world software development more accurately.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Dataset Construction -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset Construction</h2>
        <div class="content has-text-justified">
          <p>
            We gather real-world code usage from permissively licensed GitHub repositories and map each snippet to its corresponding library version 
            via <code>requirements.txt</code> and other metadata. To enable controlled experimentation, 
            we also generate synthetic tasks derived from official library documentation. 
          </p>
          <p>
            Each code completion prompt is annotated with the library version, focusing on 
            <b>introduced</b>, <b>deprecated</b>, <b>modified</b>, or <b>unchanged</b> APIs. 
            This annotation strategy allows us to assess how models handle evolving libraries in detail.
          </p>
          <p align="center">
            <!-- dataset_construction.png -->
            <img src="dataset_construction.png" class="center" width="80%" alt="[Dataset Construction Process]">
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Main Results -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results</h2>
        <div class="content has-text-justified">
          <p>
            Below we show the performance of various code LLMs (e.g., StarCoder2-7B, Mistral-7B, and GPT-4o-Mini) under three different prompt settings:
            <em>In-File (not version-aware)</em>, <em>Version-Aware</em>, and <em>Version-Aware RAG</em>. 
            The table focuses on two representative libraries: PyTorch and Matplotlib.
          </p>

          <!-- Example table with results -->
          <div class="table-container">
            <table class="table is-bordered is-striped is-fullwidth">
              <thead>
                <tr>
                  <th>Model</th>
                  <th>Completion Strategy</th>
                  <th>Context Setting</th>
                  <th>PyTorch</th>
                  <th>Matplotlib</th>
                </tr>
              </thead>
              <tbody>
                <!-- StarCoder2-7B rows -->
                <tr>
                  <td rowspan="3">Starcoder2-7B</td>
                  <td>Fill-in-the-Middle</td>
                  <td>In-File (Not Version-Aware)</td>
                  <td>68.8</td>
                  <td>69.7</td>
                </tr>
                <tr>
                  <td></td>
                  <td>+ Version-Aware</td>
                  <td>69.3</td>
                  <td>70.1</td>
                </tr>
                <tr>
                  <td></td>
                  <td>+ Version-Aware RAG</td>
                  <td><b>73.3</b></td>
                  <td><b>75.4</b></td>
                </tr>
                
                <!-- Mistral-7B rows -->
                <tr>
                  <td rowspan="3">Mistral-7B</td>
                  <td>Left-Context Only</td>
                  <td>In-File (Not Version-Aware)</td>
                  <td>65.8</td>
                  <td>60.18</td>
                </tr>
                <tr>
                  <td></td>
                  <td>+ Version-Aware</td>
                  <td>66.04</td>
                  <td>61.2</td>
                </tr>
                <tr>
                  <td></td>
                  <td>+ Version-Aware RAG</td>
                  <td><b>67.6</b></td>
                  <td><b>69.05</b></td>
                </tr>

                <!-- GPT-4o-Mini rows -->
                <tr>
                  <td rowspan="3">GPT-4o-Mini</td>
                  <td>Instruction-based (w/ Example)</td>
                  <td>In-File (Not Version-Aware)</td>
                  <td>64.3</td>
                  <td>52.5</td>
                </tr>
                <tr>
                  <td></td>
                  <td>+ Version-Aware</td>
                  <td>64.78</td>
                  <td>53.1</td>
                </tr>
                <tr>
                  <td></td>
                  <td>+ Version-Aware RAG</td>
                  <td><b>70.14</b></td>
                  <td><b>66.7</b></td>
                </tr>
              </tbody>
            </table>
          </div>

          <p>
            We observe that performance generally improves with additional version-related context or retrieval. 
            However, inherent biases in the model still remain, especially for newly introduced or long-deprecated APIs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Analysis Section -->
<section class="section" id="analysis">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Analysis</h2>
        <div class="content has-text-justified">
          <p>
            We present a collection of sub-figures that delve into crucial aspects of model performance, 
            covering scaling effects, in-file vs. version-aware retrieval, direct vs. indirect completions, and more.
          </p>
          <!-- 6 images for analysis, each about 30% width -->
          <div class="columns is-multiline">
            <div class="column is-one-third">
              <figure class="image">
                <img src="codesage_scale.png" alt="[CodeSage Scaling]" />
                <figcaption class="has-text-centered">MRR Scores for CodeSage Small and Large Models</figcaption>
              </figure>
            </div>
            <div class="column is-one-third">
              <figure class="image">
                <img src="scaling_starcoder.png" alt="[StarCoder Scaling]" />
                <figcaption class="has-text-centered">F1 Scores for Starcoder2 and StarCoder Models</figcaption>
              </figure>
            </div>
            <div class="column is-one-third">
              <figure class="image">
                <img src="infile_rag.png" alt="[In-File vs RAG]" />
                <figcaption class="has-text-centered">In-File vs Version-Aware RAG Performance</figcaption>
              </figure>
            </div>
          </div>

          <div class="columns is-multiline" style="margin-top: 15px;">
            <div class="column is-one-third">
              <figure class="image">
                <img src="direct_indirect.png" alt="[Direct vs Indirect Completions]" />
                <figcaption class="has-text-centered">Direct vs Indirect Code Completion</figcaption>
              </figure>
            </div>
            <div class="column is-one-third">
              <figure class="image">
                <img src="overall_deprecated.png" alt="[Overall vs Deprecated]" />
                <figcaption class="has-text-centered">Overall vs Deprecated Set Code Completion</figcaption>
              </figure>
            </div>
            <div class="column is-one-third">
              <figure class="image">
                <img src="rag_em.png" alt="[MRR Scores CodeSage/OpenAI Ada]" />
                <figcaption class="has-text-centered">MRR Scores for CodeSage / OpenAI Ada</figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Temporal Analysis -->
<section class="section" id="temporal-analysis">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Temporal Analysis</h2>
        <div class="content has-text-justified">
          <p>
            In addition to overall performance, we examine how models handle rapidly changing libraries over time.
            Below, three tables illustrate scenarios for <b>Matplotlib</b> (deprecated vs. introduced APIs) 
            and <b>PyTorch</b> (introduced or deprecated) from different years:
          </p>
          <div class="table-container">
            <!-- First sub-table: Matplotlib Deprecated -->
            <table class="table is-bordered is-striped is-narrow is-hoverable" style="margin-bottom: 1rem;">
              <thead>
                <tr>
                  <th colspan="3" class="has-text-centered">
                    StarCoder2 (Model Release: 2024) on Matplotlib Deprecated APIs
                  </th>
                </tr>
                <tr>
                  <th>Version Year</th>
                  <th>Deprecated API Score</th>
                  <th>Overall Score</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>2019</td>
                  <td><u>38.58</u></td>
                  <td><b>61.83</b></td>
                </tr>
                <tr>
                  <td>2020</td>
                  <td><u>43.93</u></td>
                  <td><b>60.12</b></td>
                </tr>
                <tr>
                  <td>2021</td>
                  <td>53.01</td>
                  <td><b>61.31</b></td>
                </tr>
                <tr>
                  <td>2022</td>
                  <td>52.74</td>
                  <td><b>65.15</b></td>
                </tr>
                <tr>
                  <td>2023</td>
                  <td>57.14</td>
                  <td><b>66.08</b></td>
                </tr>
              </tbody>
            </table>

            <!-- Second sub-table: Matplotlib Introduced -->
            <table class="table is-bordered is-striped is-narrow is-hoverable" style="margin-bottom: 1rem;">
              <thead>
                <tr>
                  <th colspan="3" class="has-text-centered">
                    CodeGen 1.0 (Knowledge Cutoff: 2022) on Matplotlib Introduced APIs
                  </th>
                </tr>
                <tr>
                  <th>Version Year</th>
                  <th>Introduced API Score</th>
                  <th>Overall Score</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>2020</td>
                  <td>53.14</td>
                  <td><b>62.89</b></td>
                </tr>
                <tr>
                  <td>2021</td>
                  <td>54.23</td>
                  <td><b>62.85</b></td>
                </tr>
                <tr>
                  <td>2022</td>
                  <td>56.29</td>
                  <td><b>60.04</b></td>
                </tr>
                <tr>
                  <td>2023</td>
                  <td><u>44.08</u></td>
                  <td><b>59.44</b></td>
                </tr>
                <tr>
                  <td>2024</td>
                  <td><u>41.37</u></td>
                  <td><b>58.57</b></td>
                </tr>
              </tbody>
            </table>

            <!-- Third sub-table: PyTorch Introduced/Deprecated -->
            <table class="table is-bordered is-striped is-narrow is-hoverable">
              <thead>
                <tr>
                  <th colspan="3" class="has-text-centered">
                    StarCoder2 (Knowledge Cutoff: 2024) on PyTorch Introduced/Deprecated APIs
                  </th>
                </tr>
                <tr>
                  <th>Version Year</th>
                  <th>Deprec./Intro. API Score</th>
                  <th>Overall Score</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>2018</td>
                  <td>68.78</td>
                  <td><b>71.06</b></td>
                </tr>
                <tr>
                  <td>2020</td>
                  <td>59.10</td>
                  <td><b>75.45</b></td>
                </tr>
                <tr>
                  <td>2021</td>
                  <td>68.13</td>
                  <td><b>72.84</b></td>
                </tr>
                <tr>
                  <td>2022</td>
                  <td>60.93</td>
                  <td><b>71.02</b></td>
                </tr>
                <tr>
                  <td>2023</td>
                  <td>67.13</td>
                  <td><b>72.82</b></td>
                </tr>
              </tbody>
            </table>
          </div>

          <p>
            <b>Key Takeaways:</b><br>
            - Older, deprecated APIs show a sharp decline in accuracy compared to overall performance.<br>
            - Models with earlier knowledge cutoffs (e.g., CodeGen 1.0) struggle with newly introduced APIs.<br>
            - Version-aware prompting helps but doesn’t completely resolve inherent biases; more specialized training or adaptation may be needed.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{kuhar2024libevolutioneval,
  title={{LIBEVOLUTIONEVAL}: A Benchmark and Study for Version-Specific Code Generation},
  author={Kuhar, Sachit and Ahmad, Wasi Uddin and Wang, Zijian and Jain, Nihal and Qian, Haifeng and Ray, Baishakhi and Ramanathan, Murali Krishna and Ma, Xiaofei and Deoras, Anoop},
  year={2024},
  eprint={2412.04478},
  archivePrefix={arXiv},
  primaryClass={cs.SE}
}</code></pre>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            <b>Note:</b> Our codebase and data for LIBEVOLUTIONEVAL are not publicly released yet. Stay tuned for updates.
          </p>
          <p>
            This website is licensed under a 
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
              Creative Commons Attribution-ShareAlike 4.0 International License
            </a>.
          </p>
          <p>
            You are free to reuse this layout under the same license, but please remove or update any analytics code you don’t want.
          </p>
          <p><strong>Images to Upload for This Website:</strong></p>
          <ul>
            <li><code>teaser.png</code> - Displayed at the top (TL;DR section).</li>
            <li><code>dataset_construction.png</code> - In "Dataset Construction" section.</li>
            <li><code>codesage_scale.png</code>, <code>scaling_starcoder.png</code>, <code>infile_rag.png</code>, <code>direct_indirect.png</code>, <code>overall_deprecated.png</code>, <code>rag_em.png</code> - Six images for the "Analysis" section.</li>
            <li><em>Note:</em> The “results” chart is rendered as an HTML table here instead of an image.</li>
            <li><em>Note:</em> The “temporal analysis” results are shown as HTML tables and do not require separate images. 
              If you prefer charts, you can replace them with <code>matplotlib_deprecated.png</code>, <code>matplotlib_new.png</code>, and <code>pytorch_ndm.png</code> respectively.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
